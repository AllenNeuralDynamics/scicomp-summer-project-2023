{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0691231",
   "metadata": {},
   "source": [
    "## DLC\n",
    "\n",
    "In `config.yaml` specify the body parts and the training fraction `0.8`. Label frames and saving will create a `.csv` file in the `labeled-data` folder containing the label coordinates. After labeling the extracted frames, `create training dataset` will combine the labeled datasets from all the videos and split them to create train and test datasets under `dlc-models` folder. The training data will be used to train the network, while the test data set will be used for evaluating the network. Notice that you may create multiple separate training datasets by changing the shuffle number. \n",
    "\n",
    "- Modify the training network to only do 20 iterations. You can modify the training parameters in the GUI directory or in the `dlc-models\\iteration0\\train\\pose_cfg.yaml` before training the network. \n",
    "\n",
    "Evaluate trained network will compute performance metrics such as train and test errors (**evaluation metric 1**, in `CombinedEvaluationResults` file) by comparing manual labels and predicted labels. They will be saved in the `evaluation-results` folder. \n",
    "\n",
    "Finally, you can use the network to analyze new videos (or the same video in this case). The prediction coordinates and likelihoods will be saved as an `.csv` file under the same directory as the video file. If you select the `plot trajectories` option in the GUI, it will create a `plot-poses` folder in the directory of the video, and it will contain some trajectory plots such as coordinates of body parts vs. time, likelihoods vs time.\n",
    "\n",
    "You can also create labeled videos (**evaluation metric 2**, under `videos\\mouse-top-viewDLC_resnet50_dlc-mouse-topviewJul24shuffle1_20_labeled.mp4`), modify the skeleton configurations in the `config.yaml` file to control what the skeleton should look like. \n",
    "\n",
    "- The `pcutoff` attribute in config.yaml controls the confidence / likelihood cutoff level that DLC uses to filter out predictions before creating the video. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894c195b",
   "metadata": {},
   "source": [
    "## DLC TRAINING (in progress)\n",
    "\n",
    "Either create project in Home Folder, or move to Home Folder before (or after?) generating training sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb6bf4a",
   "metadata": {},
   "source": [
    "## Format Sleap Predictions for DLC Visualiztaion (blocked)\n",
    "\n",
    "ISSUE: Cannot extract the labeled images file from .slp file, but DLC csv needs those paths and image files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3a6dc18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===filename===\n",
      "C:/Users/louise.xu/Downloads/mouse-labels.h5\n",
      "\n",
      "===HDF5 datasets===\n",
      "['edge_inds', 'edge_names', 'instance_scores', 'labels_path', 'node_names', 'point_scores', 'provenance', 'track_names', 'track_occupancy', 'tracking_scores', 'tracks', 'video_ind', 'video_path']\n",
      "\n",
      "===locations (tracks) data shape===\n",
      "(2330, 2, 2, 1)\n",
      "\n",
      "===nodes===\n",
      "0: head\n",
      "1: tailBase\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "filename = \"C:/Users/louise.xu/Downloads/mouse-labels.h5\" # or your own exported filename\n",
    "\n",
    "with h5py.File(filename, \"r\") as f:\n",
    "    dset_names = list(f.keys())\n",
    "    locations = f[\"tracks\"][:].T\n",
    "    node_names = [n.decode() for n in f[\"node_names\"][:]]\n",
    "\n",
    "    print(\"===filename===\")\n",
    "print(filename)\n",
    "print()\n",
    "\n",
    "print(\"===HDF5 datasets===\")\n",
    "print(dset_names)\n",
    "print()\n",
    "\n",
    "print(\"===locations (tracks) data shape===\")\n",
    "print(locations.shape)\n",
    "print()\n",
    "\n",
    "print(\"===nodes===\")\n",
    "for i, name in enumerate(node_names):\n",
    "    print(f\"{i}: {name}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9475ea92",
   "metadata": {},
   "source": [
    "Need to formate csv into dlc acceptable format as instructed [here](https://github.com/DeepLabCut/DeepLabCut/wiki/Using-labeled-data-in-DeepLabCut-that-was-annotated-elsewhere-(or-merge-across-labelers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e5601d97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>louise</th>\n",
       "      <th>louise</th>\n",
       "      <th>louise</th>\n",
       "      <th>louise</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>head</td>\n",
       "      <td>head</td>\n",
       "      <td>tailBase</td>\n",
       "      <td>tailBase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>x</td>\n",
       "      <td>y</td>\n",
       "      <td>x</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>76.977142</td>\n",
       "      <td>88.586418</td>\n",
       "      <td>140.419189</td>\n",
       "      <td>187.721466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>76.211014</td>\n",
       "      <td>87.9543</td>\n",
       "      <td>140.592499</td>\n",
       "      <td>180.707001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>75.789993</td>\n",
       "      <td>84.666412</td>\n",
       "      <td>143.833466</td>\n",
       "      <td>176.909729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2325</th>\n",
       "      <td>347.900238</td>\n",
       "      <td>380.163879</td>\n",
       "      <td>424.336731</td>\n",
       "      <td>443.984863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2326</th>\n",
       "      <td>349.19101</td>\n",
       "      <td>380.117371</td>\n",
       "      <td>428.229675</td>\n",
       "      <td>443.702759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2327</th>\n",
       "      <td>352.273102</td>\n",
       "      <td>380.147186</td>\n",
       "      <td>428.39679</td>\n",
       "      <td>443.756714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2328</th>\n",
       "      <td>356.354065</td>\n",
       "      <td>380.305115</td>\n",
       "      <td>432.69458</td>\n",
       "      <td>440.235352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2329</th>\n",
       "      <td>356.47879</td>\n",
       "      <td>380.162567</td>\n",
       "      <td>435.873352</td>\n",
       "      <td>439.975952</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2332 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          louise      louise      louise      louise\n",
       "0           head        head    tailBase    tailBase\n",
       "0              x           y           x           y\n",
       "0      76.977142   88.586418  140.419189  187.721466\n",
       "1      76.211014     87.9543  140.592499  180.707001\n",
       "2      75.789993   84.666412  143.833466  176.909729\n",
       "...          ...         ...         ...         ...\n",
       "2325  347.900238  380.163879  424.336731  443.984863\n",
       "2326   349.19101  380.117371  428.229675  443.702759\n",
       "2327  352.273102  380.147186   428.39679  443.756714\n",
       "2328  356.354065  380.305115   432.69458  440.235352\n",
       "2329   356.47879  380.162567  435.873352  439.975952\n",
       "\n",
       "[2332 rows x 4 columns]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "locations = locations.reshape((2330, 4))\n",
    "\n",
    "bodyparts = []\n",
    "coords = []\n",
    "scorer = []\n",
    "scorer_name = \"louise\"\n",
    "for name in node_names:\n",
    "    bodyparts += [name]*2\n",
    "    coords += [\"x\"]\n",
    "    coords += [\"y\"]\n",
    "    scorer += [scorer_name]*2\n",
    "\n",
    "def convert_df (arr):\n",
    "    return pd.DataFrame(np.array([arr]))\n",
    "\n",
    "df = pd.DataFrame(locations)\n",
    "\n",
    "df = pd.concat([convert_df(bodyparts), convert_df(coords), df])\n",
    "df.columns = scorer\n",
    "df\n",
    "#np.savetxt(\"C:/Users/louise.xu/Downloads/sleap-mouse-predictions.csv\", locations_csv, delimiter=\",\", header=header, comments=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba97d986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function create_labeled_video in module deeplabcut.utils.make_labeled_video:\n",
      "\n",
      "create_labeled_video(config, videos, videotype='', shuffle=1, trainingsetindex=0, filtered=False, fastmode=True, save_frames=False, keypoints_only=False, Frames2plot=None, displayedbodyparts='all', displayedindividuals='all', codec='mp4v', outputframerate=None, destfolder=None, draw_skeleton=False, trailpoints=0, displaycropped=False, color_by='bodypart', modelprefix='', init_weights='', track_method='', superanimal_name='', pcutoff=0.6, skeleton=[], skeleton_color='white', dotsize=8, colormap='rainbow', alphavalue=0.5, overwrite=False)\n",
      "    Labels the bodyparts in a video.\n",
      "    \n",
      "    Make sure the video is already analyzed by the function\n",
      "    ``deeplabcut.analyze_videos``.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    config : string\n",
      "        Full path of the config.yaml file.\n",
      "    \n",
      "    videos : list[str]\n",
      "        A list of strings containing the full paths to videos for analysis or a path\n",
      "        to the directory, where all the videos with same extension are stored.\n",
      "    \n",
      "    videotype: str, optional, default=\"\"\n",
      "        Checks for the extension of the video in case the input to the video is a\n",
      "        directory. Only videos with this extension are analyzed.\n",
      "        If left unspecified, videos with common extensions\n",
      "        ('avi', 'mp4', 'mov', 'mpeg', 'mkv') are kept.\n",
      "    \n",
      "    shuffle : int, optional, default=1\n",
      "        Number of shuffles of training dataset.\n",
      "    \n",
      "    trainingsetindex: int, optional, default=0\n",
      "        Integer specifying which TrainingsetFraction to use.\n",
      "        Note that TrainingFraction is a list in config.yaml.\n",
      "    \n",
      "    filtered: bool, optional, default=False\n",
      "        Boolean variable indicating if filtered output should be plotted rather than\n",
      "        frame-by-frame predictions. Filtered version can be calculated with\n",
      "        ``deeplabcut.filterpredictions``.\n",
      "    \n",
      "    fastmode: bool, optional, default=True\n",
      "        If ``True``, uses openCV (much faster but less customization of video) instead\n",
      "        of matplotlib if ``False``. You can also \"save_frames\" individually or not in\n",
      "        the matplotlib mode (if you set the \"save_frames\" variable accordingly).\n",
      "        However, using matplotlib to create the frames it therefore allows much more\n",
      "        flexible (one can set transparency of markers, crop, and easily customize).\n",
      "    \n",
      "    save_frames: bool, optional, default=False\n",
      "        If ``True``, creates each frame individual and then combines into a video.\n",
      "        Setting this to ``True`` is relatively slow as it stores all individual frames.\n",
      "    \n",
      "    keypoints_only: bool, optional, default=False\n",
      "        By default, both video frames and keypoints are visible. If ``True``, only the\n",
      "        keypoints are shown. These clips are an hommage to Johansson movies,\n",
      "        see https://www.youtube.com/watch?v=1F5ICP9SYLU and of course his seminal\n",
      "        paper: \"Visual perception of biological motion and a model for its analysis\"\n",
      "        by Gunnar Johansson in Perception & Psychophysics 1973.\n",
      "    \n",
      "    Frames2plot: List[int] or None, optional, default=None\n",
      "        If not ``None`` and ``save_frames=True`` then the frames corresponding to the\n",
      "        index will be plotted. For example, ``Frames2plot=[0,11]`` will plot the first\n",
      "        and the 12th frame.\n",
      "    \n",
      "    displayedbodyparts: list[str] or str, optional, default=\"all\"\n",
      "        This selects the body parts that are plotted in the video. If ``all``, then all\n",
      "        body parts from config.yaml are used. If a list of strings that are a subset of\n",
      "        the full list. E.g. ['hand','Joystick'] for the demo\n",
      "        Reaching-Mackenzie-2018-08-30/config.yaml to select only these body parts.\n",
      "    \n",
      "    displayedindividuals: list[str] or str, optional, default=\"all\"\n",
      "        Individuals plotted in the video.\n",
      "        By default, all individuals present in the config will be showed.\n",
      "    \n",
      "    codec: str, optional, default=\"mp4v\"\n",
      "        Codec for labeled video. For available options, see\n",
      "        http://www.fourcc.org/codecs.php. Note that this depends on your ffmpeg\n",
      "        installation.\n",
      "    \n",
      "    outputframerate: int or None, optional, default=None\n",
      "        Positive number, output frame rate for labeled video (only available for the\n",
      "        mode with saving frames.) If ``None``, which results in the original video\n",
      "        rate.\n",
      "    \n",
      "    destfolder: string or None, optional, default=None\n",
      "        Specifies the destination folder that was used for storing analysis data. If\n",
      "        ``None``, the path of the video file is used.\n",
      "    \n",
      "    draw_skeleton: bool, optional, default=False\n",
      "        If ``True`` adds a line connecting the body parts making a skeleton on each\n",
      "        frame. The body parts to be connected and the color of these connecting lines\n",
      "        are specified in the config file.\n",
      "    \n",
      "    trailpoints: int, optional, default=0\n",
      "        Number of previous frames whose body parts are plotted in a frame\n",
      "        (for displaying history).\n",
      "    \n",
      "    displaycropped: bool, optional, default=False\n",
      "        Specifies whether only cropped frame is displayed (with labels analyzed\n",
      "        therein), or the original frame with the labels analyzed in the cropped subset.\n",
      "    \n",
      "    color_by : string, optional, default='bodypart'\n",
      "        Coloring rule. By default, each bodypart is colored differently.\n",
      "        If set to 'individual', points belonging to a single individual are colored the\n",
      "        same.\n",
      "    \n",
      "    modelprefix: str, optional, default=\"\"\n",
      "        Directory containing the deeplabcut models to use when evaluating the network.\n",
      "        By default, the models are assumed to exist in the project folder.\n",
      "    \n",
      "    init_weights: str,\n",
      "        Checkpoint path to the super model\n",
      "    track_method: string, optional, default=\"\"\n",
      "        Specifies the tracker used to generate the data.\n",
      "        Empty by default (corresponding to a single animal project).\n",
      "        For multiple animals, must be either 'box', 'skeleton', or 'ellipse' and will\n",
      "        be taken from the config.yaml file if none is given.\n",
      "    \n",
      "    overwrite: bool, optional, default=False\n",
      "        If ``True`` overwrites existing labeled videos.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "        results : list[bool]\n",
      "        ``True`` if the video is successfully created for each item in ``videos``.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    \n",
      "    Create the labeled video for a single video\n",
      "    \n",
      "    >>> deeplabcut.create_labeled_video(\n",
      "            '/analysis/project/reaching-task/config.yaml',\n",
      "            ['/analysis/project/videos/reachingvideo1.avi'],\n",
      "        )\n",
      "    \n",
      "    Create the labeled video for a single video and store the individual frames\n",
      "    \n",
      "    >>> deeplabcut.create_labeled_video(\n",
      "            '/analysis/project/reaching-task/config.yaml',\n",
      "            ['/analysis/project/videos/reachingvideo1.avi'],\n",
      "            fastmode=True,\n",
      "            save_frames=True,\n",
      "        )\n",
      "    \n",
      "    Create the labeled video for multiple videos\n",
      "    \n",
      "    >>> deeplabcut.create_labeled_video(\n",
      "            '/analysis/project/reaching-task/config.yaml',\n",
      "            [\n",
      "                '/analysis/project/videos/reachingvideo1.avi',\n",
      "                '/analysis/project/videos/reachingvideo2.avi',\n",
      "            ],\n",
      "        )\n",
      "    \n",
      "    Create the labeled video for all the videos with an .avi extension in a directory.\n",
      "    \n",
      "    >>> deeplabcut.create_labeled_video(\n",
      "            '/analysis/project/reaching-task/config.yaml',\n",
      "            ['/analysis/project/videos/'],\n",
      "        )\n",
      "    \n",
      "    Create the labeled video for all the videos with an .mp4 extension in a directory.\n",
      "    \n",
      "    >>> deeplabcut.create_labeled_video(\n",
      "            '/analysis/project/reaching-task/config.yaml',\n",
      "            ['/analysis/project/videos/'],\n",
      "            videotype='mp4',\n",
      "        )\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import deeplabcut\n",
    "help(deeplabcut.create_labeled_video)\n",
    "#%cd C:\\Users\\louise.xu\\projects\\Deeplabcut\\examples\\openfield-Pranav-2018-10-30\n",
    "#deeplabcut.create_labeled_video('config.yaml', ['./videos/m3v1mp4.mp4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e457fdea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DEEPLABCUT]",
   "language": "python",
   "name": "conda-env-DEEPLABCUT-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
